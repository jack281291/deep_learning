{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:36:53.159206Z",
     "start_time": "2020-03-14T12:36:52.536120Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:37:04.043468Z",
     "start_time": "2020-03-14T12:37:04.022537Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:37:25.895420Z",
     "start_time": "2020-03-14T12:37:21.497048Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_04 import *\n",
    "torch.set_num_threads(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:53:55.578827Z",
     "start_time": "2020-03-14T12:53:53.801976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://deeplearning.net/data/mnist/mnist.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:54:12.033699Z",
     "start_time": "2020-03-14T12:54:12.004669Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize_to(train, valid):\n",
    "    m,s = train.mean(),train.std()\n",
    "    return normalize(train, m, s), normalize(valid, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:54:27.042928Z",
     "start_time": "2020-03-14T12:54:26.697317Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_valid = normalize_to(x_train,x_valid)\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:54:36.395137Z",
     "start_time": "2020-03-14T12:54:36.269311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0614e-05), tensor(1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(),x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T12:56:56.425124Z",
     "start_time": "2020-03-14T12:56:56.391300Z"
    }
   },
   "outputs": [],
   "source": [
    "nh,bs = 50,512\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To refactor layers, it's useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn.Sequential.\n",
    "\n",
    "NB: if you use a Lambda layer with a lambda function, your model won't pickle so you won't be able to save it with PyTorch. So it's best to give a name to the function you're using inside your Lambda (like flatten below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:05:08.544344Z",
     "start_time": "2020-03-14T13:05:08.520289Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def flatten(x):      return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:09:16.247255Z",
     "start_time": "2020-03-14T13:09:16.224029Z"
    }
   },
   "outputs": [],
   "source": [
    "def mnist_resize(x): return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:09:54.757933Z",
     "start_time": "2020-03-14T13:09:54.729919Z"
    }
   },
   "outputs": [],
   "source": [
    "# simple CNN\n",
    "def get_cnn_model(data):\n",
    "    return nn.Sequential(\n",
    "        Lambda(mnist_resize),\n",
    "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
    "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
    "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
    "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        Lambda(flatten),\n",
    "        nn.Linear(32,data.c)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:10:10.444258Z",
     "start_time": "2020-03-14T13:10:10.408692Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:10:15.772345Z",
     "start_time": "2020-03-14T13:10:15.753717Z"
    }
   },
   "outputs": [],
   "source": [
    "cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:10:26.478858Z",
     "start_time": "2020-03-14T13:10:26.447265Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:10:36.305837Z",
     "start_time": "2020-03-14T13:10:32.295342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.21573703125, tensor(0.1890)]\n",
      "valid: [1.2721822265625, tensor(0.5882)]\n",
      "CPU times: user 6.91 s, sys: 265 ms, total: 7.17 s\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "This took a long time to run, so it's time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:11:15.259275Z",
     "start_time": "2020-03-14T13:11:15.235422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Somewhat more flexible way\n",
    "device = torch.device('cuda',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:22:27.617467Z",
     "start_time": "2020-03-14T13:22:27.595395Z"
    }
   },
   "outputs": [],
   "source": [
    "class CudaCallback(Callback):\n",
    "    def __init__(self,device): self.device=device\n",
    "    def begin_fit(self): self.model.to(self.device)\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:23:48.961210Z",
     "start_time": "2020-03-14T13:23:48.779537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Somewhat less flexible, but quite convenient\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:25:27.380438Z",
     "start_time": "2020-03-14T13:25:27.355583Z"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:25:32.968424Z",
     "start_time": "2020-03-14T13:25:32.942227Z"
    }
   },
   "outputs": [],
   "source": [
    "cbfs.append(CudaCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:25:38.037816Z",
     "start_time": "2020-03-14T13:25:38.015631Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_cnn_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:25:44.028792Z",
     "start_time": "2020-03-14T13:25:44.005896Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=0.4)\n",
    "learn = Learner(model, opt, loss_func, data)\n",
    "run = Runner(cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T13:25:48.849863Z",
     "start_time": "2020-03-14T13:25:48.791687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.93996546875, tensor(0.3208, device='cuda:0')]\n",
      "valid: [1.04010849609375, tensor(0.6211, device='cuda:0')]\n",
      "train: [0.4293822265625, tensor(0.8685, device='cuda:0')]\n",
      "valid: [0.2628916259765625, tensor(0.9229, device='cuda:0')]\n",
      "train: [0.22102599609375, tensor(0.9330, device='cuda:0')]\n",
      "valid: [0.13378681640625, tensor(0.9600, device='cuda:0')]\n",
      "CPU times: user 5.42 s, sys: 1.14 s, total: 6.56 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model\n",
    "First we can regroup all the conv/relu in a single function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(ni, nf, ks=3, stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing is that we can do the mnist resize in a batch transform, that we can do with a Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BatchTransformXCallback(Callback):\n",
    "    _order=2\n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "    def begin_batch(self): self.run.xb = self.tfm(self.xb)\n",
    "\n",
    "def view_tfm(*size):\n",
    "    def _inner(x): return x.view(*((-1,)+size))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1,28,28)\n",
    "cbfs.append(partial(BatchTransformXCallback, mnist_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the AdaptiveAvgPool, this model can now work on any size input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8,16,32,32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_layers(data, nfs):\n",
    "    nfs = [1] + nfs\n",
    "    return [\n",
    "        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3)\n",
    "        for i in range(len(nfs)-1)\n",
    "    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n",
    "\n",
    "def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
    "    if opt_func is None: opt_func = optim.SGD\n",
    "    opt = opt_func(model.parameters(), lr=lr)\n",
    "    learn = Learner(model, opt, loss_func, data)\n",
    "    return learn, Runner(cb_funcs=listify(cbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_cnn_model(data, nfs)\n",
    "learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): AdaptiveAvgPool2d(output_size=1)\n",
       "  (5): Lambda()\n",
       "  (6): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.725136875, tensor(0.4070, device='cuda:0')]\n",
      "valid: [0.466588818359375, tensor(0.8490, device='cuda:0')]\n",
      "train: [0.32852875, tensor(0.8993, device='cuda:0')]\n",
      "valid: [0.17563607177734375, tensor(0.9480, device='cuda:0')]\n",
      "train: [0.17860630859375, tensor(0.9456, device='cuda:0')]\n",
      "valid: [0.1442845947265625, tensor(0.9557, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual insertion\n",
    "Let's say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
