{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenette data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
    "bs=128\n",
    "\n",
    "il = ImageList.from_files(path, tfms=tfms)\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [32,64,128,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback,\n",
    "        partial(BatchTransformXCallback, norm_imagenette)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.766554637171296, tensor(0.3893, device='cuda:0')]\n",
      "valid: [1.5746651074840765, tensor(0.4810, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the optimizer\n",
    "\n",
    "In PyTorch, the base optimizer in torch.optim is just a dictionary that stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups (different groups can have different learning rates/momentum/weight decay... which is what lets us do discriminative learning rates).\n",
    "\n",
    "It contains a method step that will update our parameters with the gradients and a method zero_grad to detach and zero the gradients of all our parameters.\n",
    "\n",
    "We build the equivalent from scratch, only ours will be more flexible. In our implementation, the step function loops over all the parameters to execute the step using stepper functions that we have to provide when initializing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "        self.steppers = listify(steppers)\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(-lr, p.grad.data)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Optimizer, steppers=[sgd_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have changed the optimizer, we will need to adjust the callbacks that were using properties from the PyTorch optimizer: in particular the hyper-parameters are in the list of dictionaries opt.hypers (PyTorch has everything in the the list of param groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self): plt.plot(self.lrs)\n",
    "    def plot_loss(self): plt.plot(self.losses)\n",
    "        \n",
    "    def plot(self, skip_last=0):\n",
    "        losses = [o.item() for o in self.losses]\n",
    "        n = len(losses)-skip_last\n",
    "        plt.xscale('log')\n",
    "        plt.plot(self.lrs[:n], losses[:n])\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_funcs):\n",
    "        self.pname,self.sched_funcs = pname,listify(sched_funcs)\n",
    "\n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        fs = self.sched_funcs\n",
    "        if len(fs)==1: fs = fs*len(self.opt.param_groups)\n",
    "        pos = self.n_epochs/self.epochs\n",
    "        for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)\n",
    "            \n",
    "class LR_Find(Callback):\n",
    "    _order=1\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
    "        self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr\n",
    "        self.best_loss = 1e9\n",
    "        \n",
    "    def begin_batch(self): \n",
    "        if not self.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        for pg in self.opt.hypers: pg['lr'] = lr\n",
    "            \n",
    "    def after_step(self):\n",
    "        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n",
    "            raise CancelTrainException()\n",
    "        if self.loss < self.best_loss: self.best_loss = self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback,accuracy),\n",
    "        CudaCallback, Recorder,\n",
    "        partial(ParamScheduler, 'lr', sched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.7990999825087126, tensor(0.3819, device='cuda:0')]\n",
      "valid: [1.539555384156051, tensor(0.4762, device='cuda:0')]\n",
      "CPU times: user 3.87 s, sys: 1.57 s, total: 5.44 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%time run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_decay(p, lr, wd, **kwargs):\n",
    "    p.data.mul_(1 - lr*wd)\n",
    "    return p\n",
    "weight_decay._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def l2_reg(p, lr, wd, **kwargs):\n",
    "    p.grad.data.add_(wd, p.data)\n",
    "    return p\n",
    "l2_reg._defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allow steppers to add to our defaults (which are the default values of all the hyper-parameters). This helper function adds in dest the key/values it finds while going through os and applying f when they was no key of the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def maybe_update(os, dest, f):\n",
    "    for o in os:\n",
    "        for k,v in f(o).items():\n",
    "            if k not in dest: dest[k] = v\n",
    "\n",
    "def get_defaults(d): return getattr(d,'_defaults',{})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as before, we just take the default values of the steppers when none are provided in the kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        self.steppers = listify(steppers)\n",
    "        maybe_update(self.steppers, defaults, get_defaults)\n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Optimizer():\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        self.steppers = listify(steppers)\n",
    "        maybe_update(self.steppers, defaults, get_defaults)\n",
    "        # might be a generator\n",
    "        self.param_groups = list(params)\n",
    "        # ensure params is a list of lists\n",
    "        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]\n",
    "        self.hypers = [{**defaults} for p in self.param_groups]\n",
    "\n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=sgd_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = sgd_opt(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.hypers[0]['wd'] # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.hypers[0]['lr'] # default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = sgd_opt(model.parameters(), lr=0.1, wd=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=partial(sgd_opt, wd=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.8053417731545043, tensor(0.3748, device='cuda:0')]\n",
      "valid: [1.657086733678344, tensor(0.4171, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With momentum\n",
    "\n",
    "Momentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. To do this, we introduce statistics. Statistics are object with two methods:\n",
    "\n",
    "init_state, that returns the initial state (a tensor of 0. for the moving average of gradients)\n",
    "update, that updates the state with the new gradient value\n",
    "We also read the _defaults values of those objects, to allow them to provide default values to hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatefulOptimizer(Optimizer):\n",
    "    def __init__(self, params, steppers, stats=None, **defaults): \n",
    "        self.stats = listify(stats)\n",
    "        maybe_update(self.stats, defaults, get_defaults)\n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {}\n",
    "        \n",
    "    def step(self):\n",
    "        for p,hyper in self.grad_params():\n",
    "            if p not in self.state:\n",
    "                #Create a state for p and call all the statistics to initialize it.\n",
    "                self.state[p] = {}\n",
    "                maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))\n",
    "            state = self.state[p]\n",
    "            for stat in self.stats: state = stat.update(p, state, **hyper)\n",
    "            compose(p, self.steppers, **state, **hyper)\n",
    "            self.state[p] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Stat():\n",
    "    _defaults = {}\n",
    "    def init_state(self, p): raise NotImplementedError\n",
    "    def update(self, p, state, **kwargs): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "\n",
    "    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        state['grad_avg'].mul_(mom).add_(p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    p.data.add_(-lr, grad_avg)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step,weight_decay],\n",
    "                  stats=AverageGrad(), wd=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=sgd_mom_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.0765592410101386, tensor(0.3088, device='cuda:0')]\n",
      "valid: [2.4024987559713376, tensor(0.2665, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam and friends\n",
    "\n",
    "In Adam, we use the gradient averages but with dampening (not like in SGD with momentum), so let's add this to the AverageGrad class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageGrad(Stat):\n",
    "    _defaults = dict(mom=0.9)\n",
    "    \n",
    "    def __init__(self, dampening:bool=False): self.dampening=dampening\n",
    "    def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        state['mom_damp'] = 1-mom if self.dampening else 1.\n",
    "        state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to track the moving average of the gradients squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageSqrGrad(Stat):\n",
    "    _defaults = dict(sqr_mom=0.99)\n",
    "    \n",
    "    def __init__(self, dampening:bool=True): self.dampening=dampening\n",
    "    def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}\n",
    "    def update(self, p, state, sqr_mom, **kwargs):\n",
    "        state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.\n",
    "        state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the number of steps done during training for the debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StepCount(Stat):\n",
    "    def init_state(self, p): return {'step': 0}\n",
    "    def update(self, p, state, **kwargs):\n",
    "        state['step'] += 1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function computes the debias term. If we dampening, damp = 1 - mom and we get the same result as before. If we don't use dampening, (damp = 1) we will need to divide by 1 - mom because that term is missing everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):\n",
    "    debias1 = debias(mom,     mom_damp, step)\n",
    "    debias2 = debias(sqr_mom, sqr_damp, step)\n",
    "    p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)\n",
    "    return p\n",
    "adam_step._defaults = dict(eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adam_opt(xtra_step=None, **kwargs):\n",
    "    return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),\n",
    "                   stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.001, conv_layer, cbs=cbfs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.7091763047510298, tensor(0.4128, device='cuda:0')]\n",
      "valid: [1.539838027468153, tensor(0.4917, device='cuda:0')]\n",
      "train: [1.1909504039497307, tensor(0.6077, device='cuda:0')]\n",
      "valid: [1.2981253732085987, tensor(0.5730, device='cuda:0')]\n",
      "train: [0.8826561509927131, tensor(0.7209, device='cuda:0')]\n",
      "valid: [1.296531648089172, tensor(0.5766, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(3, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 08_optimizers.ipynb to ../src/nb_08.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 \"../src/notebook2script.py\" 08_optimizers.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
