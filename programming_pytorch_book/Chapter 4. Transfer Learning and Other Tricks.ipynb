{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Transfer Learning and Other Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"data/train/\"\n",
    "\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=transforms_, is_valid_file=check_image)\n",
    "\n",
    "val_data_path = \"data/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
    "                                            transform=transforms_, is_valid_file=check_image)\n",
    "test_data_path = \"data/test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
    "                                             transform=transforms_, is_valid_file=check_image)\n",
    "\n",
    "batch_size=64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to freeze the layers. The way we do this is simple: we stop them from accumulating gradients by using requires_grad(). We need to do this for every parameter in the network, but helpfully, PyTorch provides a parameters() method that makes this rather easy:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might not want to freeze the BatchNorm layers in a model, as they will be trained to approximate the mean and standard deviation of the dataset that the model was originally trained on, not the dataset that you want to fine-tune on. \n",
    "\n",
    "Some of the signal from your data may end up being lost as BatchNorm corrects your input. You can look at the model structure and freeze only layers that aren’t BatchNorm like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the classifier\n",
    "\n",
    "Then we need to replace the final classification block with a new one that we will train for detecting cats or fish. In this example, we replace it with a couple of Linear layers, a ReLU, and Dropout, but you could have extra CNN layers here too. \n",
    "\n",
    "Happily, the definition of PyTorch’s implementation of ResNet stores the final classifier block as an instance variable, fc, so all we need to do is replace that with our new structure (other models supplied with PyTorch use either fc or classifier, so you’ll probably want to check the definition in the source if you’re trying this with a different model type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\n",
    "nn.ReLU(),                                 \n",
    "nn.Dropout(), nn.Linear(500,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.to(device)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/anton/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 20%|██        | 1/5 [00:13<00:55, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.54, Validation Loss: 0.48, accuracy = 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:20<00:34, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.19, Validation Loss: 0.19, accuracy = 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:26<00:19, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.08, Validation Loss: 0.17, accuracy = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:32<00:08,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.05, Validation Loss: 0.14, accuracy = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:39<00:00,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.04, Validation Loss: 0.14, accuracy = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=5, \n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding That Learning Rate\n",
    "\n",
    "The learning rate for training neural networks is one of the most important hyperparameters you can alter, and then waved away what you should use for it, suggesting a rather small number and for you to experiment with different values. \n",
    "\n",
    "Well…the bad news is, that really is how a lot of people discover the optimum learning rate for their architectures, usually with a technique called grid search, exhaustively searching their way through a subset of learning rate values, comparing the results against a validation dataset. This is incredibly time-consuming, and although people do it, many others err on the side of the practioner’s lore. \n",
    "\n",
    "For example, a learning rate value that has empirically been observed to work with the Adam optimizer is 3e-4. This is known as Karpathy’s constant, after Andrej Karpathy (currently director of AI at Tesla) tweeted about it in 2016. \n",
    "\n",
    "Unfortunately, fewer people read his next tweet: “I just wanted to make sure that people understand that this is a joke.” The funny thing is that 3e-4 tends to be a value that can often provide good results, so it’s a joke with a hint of reality about it.\n",
    "\n",
    "On the one hand, you have slow and cumbersome searching, and on the other, obscure and arcane knowledge gained from working on countless architectures until you get a feel for what a good learning rate would be—artisanal neural networks, even. Is there a better way than these two extremes?\n",
    "\n",
    "Thankfully, the answer is yes, although you’ll be surprised by how many people don’t use this better method. A somewhat obscure paper by Leslie Smith, a research scientist at the US Naval Research Laboratory, contained an approach for finding an appropriate learning rate.1 But it wasn’t until Jeremy Howard brought the technique to the fore in his fast.ai course that it started to catch on in the deep learning community. \n",
    "\n",
    "The idea is quite simple: over the course of an epoch, start out with a small learning rate and increase to a higher learning rate over each mini-batch, resulting in a high rate at the end of the epoch. Calculate the loss for each rate and then, looking at a plot, pick the learning rate that gives the greatest decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if(len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    if(len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(),optimizer, train_data_loader,device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Learning Rates\n",
    "\n",
    "In our training so far, we have applied one learning rate to the entire model. When training a model from scratch, that probably makes sense, but when it comes to transfer learning, we can normally get a little better accuracy if we try something different: training different groups of layers at different rates. \n",
    "\n",
    "Earlier in the chapter, we froze all the pretrained layers in our model and trained just our new classifier, but we may want to fine-tune some of the layers of, say, the ResNet model we’re using. Perhaps adding some training to the layers just preceding our classifier will make our model just a little more accurate. \n",
    "\n",
    "But as those preceding layers have already been trained on the ImageNet dataset, maybe they need only a little bit of training as compared to our newer layers? PyTorch offers a simple way of making this happen. Let’s modify our optimizer for the ResNet-50 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([\n",
    "{ 'params': transfer_model.layer4.parameters(), 'lr': found_lr /3},\n",
    "{ 'params': transfer_model.layer3.parameters(), 'lr': found_lr /9},\n",
    "{ 'params': transfer_model.fc.parameters(), 'lr': found_lr},    \n",
    "], lr=found_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 3.3333333333333335e-05\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 1.1111111111111112e-05\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That sets the learning rate for layer4 (just before our classifier) to a third of the found learning rate and a ninth for layer3. That combination has empirically worked out quite well in my work, but obviously feel free to experiment. \n",
    "\n",
    "There’s one more thing, though. As you may remember from the beginning of this chapter, we froze all these pretrained layers. It’s all very well to give them a different learning rate, but as of right now, the model training won’t touch them at all because they don’t accumulate gradients. Let’s change that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_layers = [transfer_model.layer3, transfer_model.layer4]\n",
    "for layer in unfreeze_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the parameters in these layers take gradients once more, the differential learning rates will be applied when you fine-tine the model. Note that you can freeze and unfreeze parts of the model at will and do further fine-tuning on every layer separately if you’d like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/anton/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 20%|██        | 1/5 [00:07<00:28,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.02, Validation Loss: 0.11, accuracy = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:14<00:21,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.01, Validation Loss: 0.21, accuracy = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:21<00:14,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.01, Validation Loss: 0.16, accuracy = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:27<00:06,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.01, Validation Loss: 0.13, accuracy = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.02, Validation Loss: 0.23, accuracy = 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=5, \n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "torchvision comes complete with a large collection of potential transforms that can be used for data augmentation, plus two ways of constructing new transformations. In this section, we look at the most useful ones that come supplied as well as a couple of custom transformations that you can use in your own applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                     std=[0.229, 0.224, 0.225] )\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This forms a transformation pipeline that all images go through as they enter the model for training. But the torchivision.transforms library contains many other transformation functions that can be used to augment training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ColorJitter randomly changes the brightness, contrast, saturation, and hue of an image. For brightness, contrast, and saturation, you can supply either a float or a tuple of floats, all nonnegative in the range 0 to 1, and the randomness will either be between 0 and the supplied float or it will use the tuple to generate randomness between the supplied pair of floats. For hue, a float or float tuple between –0.5 and 0.5 is required, and it will generate random hue adjustments between [-hue,hue] or [min, max]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColorJitter(brightness=None, contrast=None, saturation=None, hue=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to flip your image, these two transforms randomly reflect an image on either the horizontal or vertical axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomVerticalFlip(p=0.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.RandomHorizontalFlip(p=0.5)\n",
    "torchvision.transforms.RandomVerticalFlip(p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either supply a float from 0 to 1 for the probability of the reflection to occur or accept the default of a 50% chance of reflection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomGrayscale is a similar type of transformation, except that it randomly turns the image grayscale, depending on the parameter p (the default is 10%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomGrayscale(p=0.1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.RandomGrayscale(p=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomCrop and RandomResizeCrop, as you might expect, perform random crops on the image of size, which can either be an int for height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomResizedCrop(size=(64, 64), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.RandomCrop((64, 64), padding=None,\n",
    "pad_if_needed=False, fill=0, padding_mode='constant')\n",
    "torchvision.transforms.RandomResizedCrop((64, 64), scale=(0.08, 1.0),\n",
    "ratio=(0.75, 1.3333333333333333), interpolation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the RandomResizeCrop will resize the crop to fill the given size, RandomCrop may take a crop close to the edge and into the darkness beyond the image.\n",
    "\n",
    "RandomResizeCrop is using Bilinear interpolation, but you can also select nearest neighbor or bicubic interpolation by changing the interpolation parameter. See the PIL filters page for further details.\n",
    "\n",
    "If you’d like to randomly rotate an image, RandomRotation will vary between [-degrees, degrees] if degrees is a single float or int, or (min,max) if it is a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomRotation(degrees=(-90, 90), resample=False, expand=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.RandomRotation(90, resample=False,expand=False, center=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If expand is set to True, this function will expand the output image so that it can include the entire rotation; by default, it’s set to crop to within the input dimensions. You can specify a PIL resampling filter, and optionally provide an (x,y) tuple for the center of rotation; otherwise the transform will rotate about the center of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Spaces and Lambda Transforms\n",
    "\n",
    "This may seem a little odd to even bring up, but so far all our image work has been in the fairly standard 24-bit RGB color space, where every pixel has an 8-bit red, green, and blue value to indicate the color of that pixel. However, other color spaces are available!\n",
    "\n",
    "A popular alternative is HSV, which has three 8-bit values for hue, saturation, and value. Some people feel this system more accurately models human vision than the traditional RGB color space. But why does this matter? A mountain in RGB is a mountain in HSV, right?\n",
    "\n",
    "Well, there’s some evidence from recent deep learning work in colorization that other color spaces can produce slightly higher accuracy than RGB. A mountain may be a mountain, but the tensor that gets formed in each space’s representation will be different, and one space may capture something about your data better than another.\n",
    "\n",
    "When combined with ensembles, you could easily create a series of models that combines the results of training on RGB, HSV, YUV, and LAB color spaces to wring out a few more percentage points of accuracy from your prediction pipeline.\n",
    "\n",
    "One slight problem is that PyTorch doesn’t offer a transform that can do this. But it does provide a couple of tools that we can use to randomly change an image from standard RGB into HSV (or another color space). First, if we look in the PIL documentation, we see that we can use Image.convert() to translate a PIL image from one color space to another. \n",
    "\n",
    "We could write a custom transform class to carry out this conversion, but PyTorch adds a transforms.Lambda class so that we can easily wrap any function and make it available to the transform pipeline. Here’s our custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_colour_space(x):\n",
    "    output = x.convert(\"HSV\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then wrapped in a transforms.Lambda class and can be used in any standard transformation pipeline like we’ve seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_transform = transforms.Lambda(lambda x: _random_colour_space(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s fine if we want to convert every image into HSV, but really we don’t want that. \n",
    "We’d like it to randomly change images in each batch, so it’s probable that the image will be presented in different color spaces in different epochs. \n",
    "\n",
    "We could update our original function to generate a random number and use that to generate a random probability of changing the image, but instead we’re even lazier and use RandomApply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_colour_transform = torchvision.transforms.RandomApply([colour_transform])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, RandomApply fills in a parameter p with a value of 0.5, so there’s a 50/50 chance of the transform being applied. Experiment with adding more color spaces and the probability of applying the transformation to see what effect it has on our cat and fish problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transform Classes\n",
    "\n",
    "Sometimes a simple lambda isn’t enough; maybe we have some initialization or state that we want to keep track of, for example. In these cases, we can create a custom transform that operates on either PIL image data or a tensor. \n",
    "\n",
    "Such a class has to implement two methods: __call__, which the transform pipeline will invoke during the transformation process; and __repr__, which should return a string representation of the transform, along with any state that may be useful for diagnostic purposes.\n",
    "\n",
    "In the following code, we implement a transform class that adds random Gaussian noise to a tensor. When the class is initialized, we pass in the mean and standard distribution of the noise we require, and during the __call__ method, we sample from this distribution and add it to the incoming tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "    \n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        return tensor.add_(noise)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__  }(mean={self.mean},sttdev={self.stddev})\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add this to a pipeline, we can see the results of the __repr__ method being called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform_pipeline = transforms.Compose([random_colour_transform, Noise(0.1, 0.05)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomApply(\n",
       "    p=0.5\n",
       "    Lambda()\n",
       ")\n",
       "    Noise(mean=0.1,sttdev=0.05)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_transform_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because transforms don’t have any restrictions and just inherit from the base Python object class, you can do anything. Want to completely replace an image at runtime with something from Google image search? Run the image through a completely different neural network and pass that result down the pipeline? And so on.\n",
    "\n",
    "Aside from transformations, there are a few more ways of squeezing as much performance from a model as possible. Let’s look at more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Small and Get Bigger!\n",
    "\n",
    "Here’s a tip that seems odd, but obtains real results: start small and get bigger. What I mean is if you’re training on 256 × 256 images, create a few more datasets in which the images have been scaled to 64 × 64 and 128 × 128. Create your model with the 64 × 64 dataset, fine-tune as normal, and then train the exact same model with the 128 × 128 dataset. Not from scratch, but using the parameters that have already been trained. Once it looks like you’ve squeezed the most out of the 128 × 128 data, move on to your target 256 × 256 data. You’ll probably find a percentage point or two improvement in accuracy.\n",
    "\n",
    "While we don’t know exactly why this works, the working theory is that by training at the lower resolutions, the model learns about the overall structure of the image and can refine that knowledge as the incoming images expand. But that’s just a theory. However, that doesn’t stop it from being a good little trick to have up your sleeve when you need to squeeze every last bit of performance from a model.\n",
    "\n",
    "If you don’t want to have multiple copies of a dataset hanging around in storage, you can use torchvision transforms to do this on the fly using the Resize function:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resize = transforms.Compose([ transforms.Resize((64, 64)),\n",
    " …_other augmentation transforms_…\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty you pay here is that you end up spending more time in training, as PyTorch has to apply the resize every time. If you resized all the images beforehand, you’d likely get a quicker training run, at the expense of filling up your hard drive. But isn’t that trade-off always the way?\n",
    "\n",
    "The concept of starting small and then getting bigger also applies to architectures. Using a ResNet architecture like ResNet-18 or ResNet-34 to test out approaches to transforms and get a feel for how training is working provides a much tighter feedback loop than if you start out using a ResNet-101 or ResNet-152 model. Start small, build upward, and you can potentially reuse the smaller model runs at prediction time by adding them to an ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "What’s better than one model making predictions? Well, how about a bunch of them? Ensembling is a technique that is fairly common in more traditional machine learning methods, and it works rather well in deep learning too. The idea is to obtain a prediction from a series of models, and combine those predictions to produce a final answer. Because different models will have different strengths in different areas, hopefully a combination of all their predictions will produce a more accurate result than one model alone.\n",
    "\n",
    "There are plenty of approaches to ensembles, and we won’t go into all of them here. Instead, here’s a simple way of getting started with ensembles, one that has eeked out another 1% of accuracy in my experience; simply average the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]\n",
    "predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] \n",
    "avg_prediction = torch.stack(predictions).mean(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(421, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0005, 0.0008, 0.0022,  ..., 0.0006, 0.0003, 0.0020]],\n",
       "\n",
       "        [[0.0008, 0.0007, 0.0006,  ..., 0.0010, 0.0005, 0.0004]]],\n",
       "       device='cuda:0', grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stack method concatenates the array of tensors together, so if we were working on the cat/fish problem and had our models in our ensemble, we’d end up with a 4 × 2 tensor constructed from the four 1 × 2 tensors. And mean does what you’d expect, taking the average, although we have to pass in a dimension of 0 to ensure that it takes an average across the first dimension instead of simply adding up all the tensor elements and producing a scalar output. Finally, argmax picks out the tensor index with the highest element, as you’ve seen before.\n",
    "\n",
    "It’s easy to imagine more complex approaches. Perhaps weights could be added to each individual model’s prediction, and those weights adjusted if a model gets an answer right or wrong. What models should you use? I’ve found that a combination of ResNets (e.g., 34, 50, 101) work quite well, and there’s nothing to stop you from saving your model regularly and using different snapshots of the model across time in your ensemble!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
