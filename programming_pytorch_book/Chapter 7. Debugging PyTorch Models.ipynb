{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Debugging PyTorch Models\n",
    "\n",
    "We’ve created a lot of models so far in this book, but in this chapter, we have a brief look at interpreting them and working out what’s going on underneath the covers. \n",
    "\n",
    "We take a look at using class activation mapping with PyTorch hooks to determine the focus of a model’s decision about how to connect PyTorch to Google’s TensorBoard for debugging purposes. \n",
    "\n",
    "I show how to use flame graphs to identify the bottlenecks in transforms and training pipelines, as well as provide a worked example of speeding up a slow transformation. Finally, we look at how to trade compute for memory when working with larger models using checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "TensorBoard is a web application designed for visualizing various aspects of neural networks. It allows for easy, real-time viewing of statistics such as accuracy, losses activation values, and really anything you want to send across the wire. \n",
    "\n",
    "Although it was written with TensorFlow in mind, it has such an agnostic and fairly straightforward API that working with it in PyTorch is not that different from how you’d use it in TensorFlow.\n",
    "\n",
    "TensorBoard can then be started on the command line:\n",
    "\n",
    "tensorboard --logdir=runs\n",
    "\n",
    "You can then go to http://[your-machine]:6006, where you’ll see the welcome screen shown in Figure 7-1. We can now send data to the application.”\n",
    "\n",
    "Excerpt From: Ian Pointer. “Programming PyTorch for Deep Learning”. Apple Books. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Data to TensorBoard\n",
    "\n",
    "The module for using TensorBoard with PyTorch is located in torch.utils.tensorboard:\n",
    "\n",
    "- from torch.utils.tensorboard import SummaryWriter\n",
    "- writer = SummaryWriter()\n",
    "- writer.add_scalar('example', 3)\n",
    "\n",
    "We use the SummaryWriter class to talk to TensorBoard using the standard location for logging output, ./runs, and we can send a scalar by using add_scalar with a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer.add_scalar('example', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "value = 10\n",
    "writer.add_scalar('test_loop', value, 0)\n",
    "for i in range(1,10000):\n",
    "  value += random.random() - 0.5\n",
    "  writer.add_scalar('test_loop', value, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to replace our print statements in the training loop. We can also send the model itself to get a representation in TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms,models\n",
    "\n",
    "writer = SummaryWriter()\n",
    "model = models.resnet18(False)\n",
    "writer.add_graph(model,torch.rand([1,3,224,224]))\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_data_loader, test_data_loader, epochs=20):\n",
    "    model = model.train()\n",
    "    iteration = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            writer.add_scalar('loss', loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            input, target = batch\n",
    "            output = model(input)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "            print(\"Epoch {}, accuracy = {:.2f}\".format(epoch,\n",
    "                   num_correct / num_examples))\n",
    "            writer.add_scalar('accuracy', num_correct / num_examples, epoch)\n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the ability to send accuracy and loss information as well as model structure to TensorBoard. By aggregating multiple runs of accuracy and loss information, we can see whether anything is different in a particular run compared to others, which is a useful clue when trying to work out why a training run produced poor results. We return to TensorBoard shortly, but first let’s look at other features that PyTorch makes available for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Hooks\n",
    "\n",
    "PyTorch has hooks, which are functions that can be attached to either a tensor or a module on the forward or backward pass. \n",
    "\n",
    "When PyTorch encounters a module with a hook during a pass, it will call the registered hooks. A hook registered on a tensor will be called when its gradient is being calculated.\n",
    "\n",
    "Hooks are potentially powerful ways of manipulating modules and tensors because you can completely replace the output of what comes into the hook if you so desire. You could change the gradient, mask off activations, replace all the biases in the module, and so on. In this chapter, though, we’re just going to use them as a way of obtaining information about the network as data flows through.\n",
    "\n",
    "Given a ResNet-18 model, we can attach a forward hook on a particular part of the model by using register_forward_hook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input is torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4940e-01,  2.5545e-01, -1.3384e-01, -6.8795e-01, -3.1285e-01,\n",
       "         -2.4005e-01, -7.4302e-02,  4.4725e-01, -5.0306e-01,  7.5905e-01,\n",
       "         -2.1913e-01, -9.9590e-01, -6.6987e-01, -7.4829e-01,  1.7848e-01,\n",
       "          4.7451e-01,  8.6537e-01,  6.0188e-01,  2.5773e-01,  5.6054e-02,\n",
       "          4.7870e-01,  1.8389e-01, -1.1972e-01, -6.7993e-01,  2.5985e-01,\n",
       "          3.2601e-01, -3.3697e-01, -1.5563e-03,  2.2842e-01,  4.9729e-02,\n",
       "          4.8531e-01,  2.3919e-01,  2.1326e-02, -6.2627e-01, -3.1924e-01,\n",
       "          2.2056e-01,  2.4207e-01,  2.3682e-02, -2.6045e-01,  4.1044e-01,\n",
       "         -4.3596e-01, -8.5637e-03,  3.1259e-03,  3.7924e-01, -1.1541e-01,\n",
       "         -3.7762e-01,  6.0185e-01,  5.2089e-02,  3.0902e-01, -3.4481e-01,\n",
       "          3.6467e-01,  2.7574e-01, -7.7089e-01, -6.8905e-02,  2.4259e-01,\n",
       "         -6.1318e-01, -1.2381e-01,  7.0634e-01,  4.9156e-01, -1.0248e+00,\n",
       "          1.0890e+00,  8.6883e-01,  6.0189e-01,  5.8971e-01, -1.5924e-01,\n",
       "          5.1269e-01, -2.9370e-01,  2.2631e-01, -4.6067e-01, -4.2806e-01,\n",
       "          2.8925e-01, -5.4241e-01, -1.6797e-01, -1.2849e-01, -2.5816e-01,\n",
       "         -1.9289e-01, -4.8484e-01, -1.0547e+00,  1.2798e-01, -1.5071e-01,\n",
       "          1.1858e+00, -8.2985e-01, -4.0950e-01,  2.0873e-01, -9.0101e-01,\n",
       "         -4.5999e-01,  2.7083e-01,  8.1741e-01,  1.2540e-01, -3.3758e-01,\n",
       "          1.8623e-01, -1.4662e-01,  3.3048e-01, -4.1166e-01, -2.8127e-02,\n",
       "          1.7819e-01,  3.6209e-01,  7.5412e-01, -1.0948e+00,  8.2597e-02,\n",
       "          1.2959e-01,  7.1251e-01, -1.8545e-01, -7.8143e-01,  2.1173e-01,\n",
       "          1.1588e+00,  8.0275e-02,  4.7126e-01,  9.7768e-02,  6.2204e-01,\n",
       "         -6.6258e-01,  5.9700e-01,  4.9799e-01, -3.6287e-01,  5.4744e-01,\n",
       "          1.4786e-02,  3.2756e-01,  2.7578e-02, -1.4972e-01, -7.8620e-01,\n",
       "          1.5789e+00, -6.1922e-02, -5.3054e-01, -1.0967e-01,  1.5333e-02,\n",
       "         -2.2474e-03, -7.7888e-01,  1.0343e+00, -4.8539e-02,  2.3480e-01,\n",
       "         -4.6474e-01, -1.3998e-01, -4.2989e-01, -5.7317e-01,  2.1045e-01,\n",
       "         -5.8527e-01,  3.0588e-01, -4.8516e-01, -1.1716e-02,  6.1327e-01,\n",
       "         -7.1661e-01,  2.3830e-01, -5.5987e-02,  8.3266e-01, -3.7249e-01,\n",
       "          8.4646e-01, -1.6278e-01, -2.5098e-01,  5.2809e-01,  4.0139e-01,\n",
       "         -3.6981e-01, -1.7488e-02, -1.7649e-01, -1.7934e-01,  3.9535e-01,\n",
       "          1.1918e+00,  3.9710e-01, -1.5622e-01, -6.1923e-01,  2.9894e-01,\n",
       "         -4.3490e-01, -1.3127e+00, -2.6643e-01,  1.5108e-01, -4.9763e-01,\n",
       "          3.8315e-01,  3.4594e-01,  5.4979e-01, -3.3766e-01, -2.4615e-01,\n",
       "         -1.8375e-01,  7.9321e-02,  8.9189e-02,  2.2181e-01,  2.8633e-01,\n",
       "          7.2197e-01,  4.9362e-01,  7.1194e-02,  2.8362e-01, -3.0564e-01,\n",
       "          9.0313e-01, -3.0493e-01,  2.7039e-01,  3.5457e-01, -3.5424e-01,\n",
       "         -5.7614e-01,  5.6200e-01, -4.7408e-03, -4.0976e-01, -3.5584e-01,\n",
       "          1.5476e-01,  1.2969e-01, -3.7643e-01,  1.6255e-01,  1.8902e-01,\n",
       "         -4.5148e-02, -3.1769e-01,  4.9140e-01, -1.4808e-01, -2.4541e-01,\n",
       "          3.1873e-01,  1.4589e-01, -6.2903e-01, -2.1572e-01,  8.6306e-02,\n",
       "         -6.7494e-03, -7.9650e-01,  1.6352e-01, -3.7582e-01, -1.0050e+00,\n",
       "          4.9425e-02, -2.3784e-01,  7.8649e-02, -5.7485e-01,  4.6217e-01,\n",
       "          8.2114e-01,  5.0761e-01, -6.2257e-02,  9.1452e-02, -3.0643e-01,\n",
       "          3.9272e-01, -6.3673e-02,  4.1397e-01,  2.0924e-01,  7.6589e-02,\n",
       "          4.2692e-03,  3.6887e-01, -1.0130e+00, -1.5363e-01,  6.3628e-02,\n",
       "          2.6060e-01, -1.2621e-01,  3.4438e-01, -5.8700e-02, -1.9986e-01,\n",
       "          4.0451e-01,  6.3271e-01,  3.8835e-01, -1.3494e-01, -2.4416e-01,\n",
       "         -1.4058e-01, -9.5130e-01,  1.1622e+00,  2.5094e-01,  5.6391e-01,\n",
       "          3.0039e-01,  8.0759e-02,  2.7639e-01,  1.8753e-01,  4.3871e-01,\n",
       "          2.0634e-01,  1.0339e+00,  5.4851e-01, -4.5840e-01, -1.2242e-01,\n",
       "          2.2513e-01,  6.0006e-01, -6.5126e-01,  6.3609e-01, -2.1610e-01,\n",
       "          6.8400e-01,  7.4715e-01,  8.8004e-01,  8.5170e-01, -4.0845e-01,\n",
       "         -1.6757e-01, -6.8810e-05, -1.8135e-01, -1.9100e-01, -9.4670e-01,\n",
       "         -4.9372e-01, -4.1851e-01, -2.3383e-01, -4.6547e-01,  5.1438e-01,\n",
       "         -2.4540e-02, -8.1628e-01,  2.5756e-01, -3.2152e-01, -6.7062e-01,\n",
       "         -4.2682e-01,  6.0701e-02, -6.2060e-01,  2.4576e-01,  5.2477e-01,\n",
       "          6.9455e-01,  1.9885e-01,  3.2699e-01, -8.4168e-01,  9.9146e-01,\n",
       "         -4.6625e-01, -3.3852e-01,  5.1076e-01, -1.2189e-01,  6.9944e-01,\n",
       "         -9.1801e-01, -1.9087e-01, -2.1465e-01, -6.4942e-01,  3.2747e-01,\n",
       "          7.1013e-01,  2.6762e-01, -1.5769e-01,  5.6944e-01,  4.8893e-01,\n",
       "         -4.1666e-01,  1.0450e+00, -5.8896e-01,  1.9123e-01,  5.3263e-01,\n",
       "         -8.0863e-01, -7.8446e-01, -3.0427e-01,  1.6171e-01, -3.1136e-01,\n",
       "         -3.6446e-01,  6.0405e-01,  1.0919e-01, -6.5892e-01, -5.1328e-01,\n",
       "          3.7737e-01,  6.2740e-02, -5.9027e-01, -6.1713e-01,  2.4536e-01,\n",
       "          2.8719e-01, -2.8687e-01,  2.5857e-01, -1.1269e-02, -1.1301e-01,\n",
       "         -1.2327e-01, -4.9828e-01,  5.2824e-01, -2.2477e-01, -5.4925e-01,\n",
       "          7.8434e-01, -7.5357e-01,  5.6968e-01, -6.6382e-01,  3.3768e-01,\n",
       "         -3.7684e-01,  1.3799e-01, -9.0550e-01, -1.1403e-01, -2.8905e-01,\n",
       "          1.8692e-01, -1.5719e-01,  3.3760e-01,  9.2812e-01,  7.6056e-02,\n",
       "          1.7822e-02, -8.9757e-02,  6.1409e-01,  3.0428e-01, -7.4143e-01,\n",
       "          4.4134e-03, -4.0038e-01, -1.3912e-01,  3.2679e-01,  4.9753e-02,\n",
       "         -2.0302e-01, -2.9163e-02, -5.4039e-01, -1.9009e-02, -1.6282e-01,\n",
       "          3.0887e-02, -5.5310e-02, -1.1500e-01, -8.9411e-01, -3.6111e-01,\n",
       "         -3.3223e-01, -1.4870e-01, -1.7239e-01,  1.9536e-01,  2.6673e-01,\n",
       "          7.8297e-01,  5.3647e-01,  2.4631e-01, -9.2612e-01,  4.0243e-01,\n",
       "         -3.1538e-02,  5.4106e-01,  1.7132e-01, -2.2698e-01, -1.2245e-01,\n",
       "          4.5485e-01, -9.3354e-01, -3.7422e-01, -4.6209e-01, -8.6909e-01,\n",
       "          9.6150e-01, -2.3061e-01, -9.0137e-01, -2.3799e-01,  1.0295e+00,\n",
       "         -3.5359e-01,  3.2199e-01,  2.0790e-01,  5.1472e-02, -2.5371e-01,\n",
       "          2.9817e-02, -9.1771e-01, -3.7491e-01, -2.2396e-01,  1.5144e-01,\n",
       "          1.3356e-01, -4.5108e-01,  6.1333e-01, -2.8898e-01, -9.0994e-01,\n",
       "         -4.0295e-02, -5.7767e-01, -6.8023e-01, -4.6533e-01, -3.9000e-01,\n",
       "         -2.4790e-01,  2.1679e-01, -8.3237e-01, -5.5646e-01,  6.0187e-02,\n",
       "         -4.2661e-01,  4.8360e-01, -3.4285e-01,  5.0973e-01, -4.1868e-01,\n",
       "         -7.9815e-01, -7.6885e-01,  1.0330e+00,  5.9384e-02, -4.8251e-01,\n",
       "         -3.4282e-02,  8.0633e-02,  3.8044e-01, -1.7454e-01, -2.3110e-03,\n",
       "          4.9217e-01,  8.3702e-01,  3.9713e-01,  4.5615e-01, -4.1015e-01,\n",
       "          3.6747e-01, -7.2887e-02,  2.6174e-01,  7.1136e-01, -3.7467e-01,\n",
       "         -5.8759e-01, -1.6745e-01,  2.2318e-01, -4.8789e-01, -3.2679e-01,\n",
       "         -1.9618e-01, -4.0890e-01, -2.6402e-01, -6.8438e-03, -1.5355e-01,\n",
       "         -1.5942e-01,  7.3677e-01, -8.2040e-01,  2.4952e-01, -4.2384e-01,\n",
       "          2.6459e-01,  2.6886e-01,  7.3337e-02, -9.8758e-01,  3.0389e-01,\n",
       "         -1.9584e-01,  3.8962e-01,  5.4045e-01,  2.8018e-01, -2.2452e-01,\n",
       "         -2.2238e-01, -1.1722e-01,  5.5809e-02,  6.9298e-02, -6.5660e-01,\n",
       "          6.5480e-02, -4.2887e-01, -5.2432e-01,  2.7098e-01,  1.2113e+00,\n",
       "          4.1450e-01, -5.3779e-01,  6.1444e-01, -2.6295e-01,  3.7613e-01,\n",
       "          5.4026e-01,  2.1541e-03, -9.9269e-02, -2.6673e-02, -8.1850e-02,\n",
       "         -9.6217e-01,  2.7298e-01, -1.5718e-01, -7.0432e-01,  3.3801e-01,\n",
       "          3.2520e-01, -8.2327e-02,  6.9186e-01, -4.5866e-01,  2.7777e-01,\n",
       "          2.5470e-01, -3.9755e-01,  1.1888e-01, -8.7942e-01, -6.1067e-01,\n",
       "         -4.2077e-01, -1.5625e-01,  9.4934e-01,  6.8051e-02,  7.1254e-01,\n",
       "         -1.6975e-01, -5.4908e-02, -6.6679e-01, -6.4781e-01,  5.6388e-01,\n",
       "         -3.9284e-01, -3.9441e-01, -1.0682e+00,  9.6155e-02, -5.6096e-01,\n",
       "         -2.5008e-01,  3.0186e-01, -5.4959e-01,  5.9753e-01, -1.1229e+00,\n",
       "         -3.2723e-01,  6.9904e-01, -6.1618e-01, -2.9376e-01,  6.4193e-01,\n",
       "         -7.1887e-01,  5.0155e-03,  1.4969e-01, -1.0280e+00, -2.0596e-01,\n",
       "         -7.8943e-01, -2.7028e-01, -5.9889e-01, -1.7542e-01, -6.7462e-01,\n",
       "          7.1254e-01, -5.2983e-01, -3.8595e-02, -9.9412e-01, -9.3259e-02,\n",
       "         -6.0343e-01, -1.3658e+00,  2.3995e-01,  3.1990e-01, -2.3661e-01,\n",
       "          7.8546e-02, -9.0596e-01, -6.0754e-01,  6.7459e-01,  8.2171e-01,\n",
       "         -3.8257e-01, -4.1019e-01,  7.8121e-01, -5.6560e-01, -1.5799e-01,\n",
       "          2.4446e-01, -6.3929e-01,  2.2941e-01,  2.2702e-01, -6.5997e-01,\n",
       "         -3.3198e-01,  1.8314e-01, -1.1511e-01,  2.9059e-01, -2.0087e-02,\n",
       "         -1.7022e-01,  1.5077e-02, -3.6610e-02,  1.1488e-01,  8.9739e-01,\n",
       "         -3.4846e-02, -9.4661e-01,  4.8554e-01,  6.5324e-01,  3.4358e-01,\n",
       "         -2.2618e-01,  7.7383e-01,  1.5524e-01,  3.0648e-01,  2.0630e-01,\n",
       "         -6.5523e-02, -8.7753e-02, -5.9396e-01,  9.4994e-02,  2.6795e-01,\n",
       "          6.6536e-01, -7.9431e-02,  4.0629e-01,  9.4226e-01,  1.7447e-01,\n",
       "          5.1440e-01,  5.6658e-01,  1.4113e-01,  1.7750e-01, -7.3116e-01,\n",
       "          7.3670e-01,  3.0974e-01, -7.6535e-02, -1.2362e-01, -4.0085e-01,\n",
       "          1.6654e-01, -4.7099e-01, -1.1309e-02,  6.2746e-01, -2.0248e-01,\n",
       "         -7.4123e-02, -5.7549e-01, -2.3974e-01,  2.1513e-01, -2.8886e-01,\n",
       "          4.6201e-01, -7.5281e-01,  5.2854e-02,  5.6308e-01,  2.2208e-01,\n",
       "         -3.4125e-01,  2.4649e-01,  1.0986e+00, -3.4759e-01,  9.7079e-02,\n",
       "         -7.5394e-01, -5.6206e-01,  3.4635e-01, -1.6265e-01, -1.2574e-01,\n",
       "         -3.1879e-01, -4.9459e-02,  2.6057e-01,  4.8297e-01,  3.0769e-01,\n",
       "          4.4073e-01, -2.4908e-01,  3.7911e-01, -2.6484e-01,  1.1906e-01,\n",
       "          5.3612e-01, -9.2233e-01,  5.9224e-01,  1.0314e-01, -8.7212e-02,\n",
       "          8.3443e-01, -2.1075e-01,  1.5590e-01, -7.9317e-01,  2.9423e-01,\n",
       "         -2.6590e-02,  3.5597e-01,  4.9922e-01, -8.5720e-01,  4.0450e-01,\n",
       "         -3.4752e-01, -9.6955e-01, -1.2687e-01, -2.0550e-01, -1.0810e-01,\n",
       "          4.3605e-01, -9.5500e-02, -4.2461e-02, -5.3584e-01, -9.0771e-02,\n",
       "          5.0698e-01,  5.7550e-01, -1.3395e+00, -6.0356e-01, -7.9359e-01,\n",
       "          6.2212e-01, -8.3466e-01,  2.1818e-01, -2.3194e-01,  1.1297e-01,\n",
       "          5.0712e-01, -1.5186e-01,  2.8148e-01, -5.1242e-02,  3.1601e-01,\n",
       "         -4.8372e-01,  1.2668e-01, -9.8093e-01,  4.7590e-01, -5.8346e-02,\n",
       "          2.3739e-01, -7.8793e-01, -5.1975e-01,  2.5247e-01, -7.8221e-02,\n",
       "          1.0298e-01,  2.2617e-01,  3.5658e-01,  1.3803e-01,  3.9136e-01,\n",
       "          2.7593e-02, -8.9899e-02, -4.5996e-01, -1.8965e-01, -2.6694e-01,\n",
       "         -3.3037e-01, -1.1310e+00,  3.5315e-01, -1.4626e-01, -6.3986e-01,\n",
       "         -8.8246e-02, -7.3180e-01,  4.9328e-01, -1.3609e-01,  6.6078e-01,\n",
       "          1.1066e-01, -3.6594e-01, -2.5208e-01, -6.3468e-02, -8.7772e-01,\n",
       "          7.4300e-02, -2.9992e-01, -5.2496e-01,  5.7225e-01, -4.8128e-03,\n",
       "         -5.4941e-01,  6.0422e-01, -7.6770e-01, -3.0111e-01,  6.3247e-01,\n",
       "         -1.5288e+00, -2.5853e-01,  3.4177e-02,  5.1568e-01,  4.6526e-01,\n",
       "          9.2664e-01,  8.0037e-01, -2.7596e-01,  4.9178e-01, -3.3005e-01,\n",
       "          4.9895e-01,  3.8885e-01, -1.4578e-01,  1.8846e-01,  3.0273e-01,\n",
       "          5.2717e-01,  5.3341e-01,  2.6439e-01, -4.5929e-01, -6.1091e-01,\n",
       "          2.0833e-01, -6.2872e-01,  2.7280e-01,  1.9550e-01, -7.2325e-01,\n",
       "          2.8520e-01, -1.8645e-01,  7.6218e-01, -2.2406e-02, -2.0948e-01,\n",
       "         -1.0133e+00,  6.9913e-01, -3.9506e-02, -4.1563e-01,  8.6280e-01,\n",
       "         -6.9516e-01, -4.5118e-01, -3.6649e-01,  4.3915e-01,  3.2745e-01,\n",
       "         -6.9277e-02, -5.8098e-02,  1.2561e+00, -3.5753e-01,  3.2098e-01,\n",
       "          6.5463e-01, -2.0995e-01,  1.9535e-01,  2.0049e-01, -8.2776e-02,\n",
       "          1.1565e+00, -2.0745e-01,  6.2236e-01,  2.6074e-01, -4.7827e-01,\n",
       "          6.2697e-01, -7.7877e-02, -9.4330e-01,  4.8984e-01, -4.5966e-01,\n",
       "          5.4231e-01,  5.7894e-01, -6.9320e-01,  3.3594e-01,  1.0810e-01,\n",
       "         -3.6550e-01, -9.3238e-01, -8.4363e-02, -8.9478e-01, -8.4830e-01,\n",
       "         -2.5649e-01, -2.2449e-01,  7.2396e-01,  7.2727e-01,  9.5433e-02,\n",
       "          2.5290e-01, -4.8915e-01, -1.3391e-01,  1.4614e-02,  6.2335e-01,\n",
       "          8.3685e-01,  2.5699e-01,  4.7092e-01,  1.8266e-01, -1.9239e-01,\n",
       "         -8.4957e-02,  5.8160e-01, -3.4046e-01,  7.0260e-02,  1.1379e+00,\n",
       "         -1.3505e-01, -4.2644e-01,  1.8344e-01,  1.2355e+00, -8.3614e-01,\n",
       "         -3.9751e-02, -2.4336e-01, -9.6815e-01,  3.9312e-02, -8.5327e-01,\n",
       "          6.5993e-02,  7.7845e-01, -1.8965e-01, -5.1434e-01, -5.6892e-01,\n",
       "         -9.7841e-02, -8.4982e-01,  1.7695e-01,  9.8695e-01,  2.6705e-01,\n",
       "          1.9128e-01,  9.1501e-01, -5.3764e-01, -3.2347e-01,  6.4773e-01,\n",
       "          2.2922e-01,  1.2498e-01,  2.0397e-01,  1.5401e-01, -3.6029e-01,\n",
       "         -5.8343e-01,  2.4922e-01, -3.8247e-01,  1.1995e-01,  1.9383e-01,\n",
       "         -6.8020e-01,  2.3995e-01,  3.7995e-01, -3.5266e-01,  4.3209e-01,\n",
       "          3.2967e-02, -2.1606e-01,  4.0606e-01, -2.3049e-02,  7.2094e-01,\n",
       "          1.0194e-01,  5.5517e-01,  1.4790e-01,  2.1595e-02, -6.6256e-01,\n",
       "          4.5645e-01, -1.0825e-01, -1.6517e-01, -3.1147e-01, -6.2311e-01,\n",
       "          3.3587e-01, -9.5913e-01,  8.8956e-01,  7.1151e-01, -1.2870e-02,\n",
       "          2.9991e-01,  7.9349e-03, -1.9132e-01,  4.6350e-01,  1.0337e-01,\n",
       "         -6.3104e-01,  6.2577e-01, -4.6632e-02,  3.7540e-01, -4.6653e-01,\n",
       "          6.4628e-01,  8.4648e-01,  9.0575e-01, -3.0023e-02,  1.3690e-01,\n",
       "         -3.9622e-01, -4.5419e-02,  2.7052e-01,  5.9225e-01, -2.7148e-01,\n",
       "          9.0250e-01,  2.1817e-01, -5.4674e-01,  3.8886e-02, -1.4247e-01,\n",
       "         -4.2446e-02,  3.8313e-02,  1.1182e+00, -2.4381e-01, -2.5444e-01,\n",
       "          5.5962e-01, -6.3929e-01, -8.8859e-01, -3.2801e-02,  1.8092e-01,\n",
       "         -8.7278e-01, -1.1349e-02, -3.2452e-01,  7.7929e-01, -6.1551e-01,\n",
       "         -1.4598e-01,  1.2353e-01, -7.1593e-02, -7.6908e-01, -1.4988e-01,\n",
       "         -1.4611e-01, -9.0146e-01, -3.1866e-01,  3.6685e-01, -6.1260e-02,\n",
       "          1.0158e-01,  2.7379e-02, -8.0992e-03,  4.1648e-01,  8.5330e-01,\n",
       "         -2.2436e-01, -6.9962e-01, -8.3086e-01, -9.8342e-01,  6.9350e-02,\n",
       "          2.9158e-01,  2.5391e-01,  5.3657e-01,  3.8554e-01, -9.4718e-02,\n",
       "         -3.6749e-01, -1.9064e-02, -9.5235e-02, -5.6337e-01, -3.1055e-01,\n",
       "         -5.1534e-01,  1.0316e-01,  2.6360e-01, -5.8483e-01,  1.6777e-01,\n",
       "          1.9597e+00, -8.9015e-01, -6.8608e-01,  4.4892e-01, -4.2718e-01,\n",
       "          5.5666e-01, -1.3265e-01,  2.8063e-02,  2.6505e-01,  2.7401e-02,\n",
       "          4.6337e-02,  4.3976e-01,  9.5604e-01,  6.9241e-01,  6.0205e-01,\n",
       "          5.4172e-01, -2.7253e-01, -1.0333e-01,  2.8971e-01,  5.5078e-01,\n",
       "          1.3985e-01,  2.6209e-01, -1.1475e-01, -4.9153e-01,  4.6264e-01,\n",
       "          1.3022e-01, -8.8481e-01,  1.3563e-01,  1.2940e+00, -4.2423e-01,\n",
       "          1.4811e-01, -1.1017e+00,  6.7191e-01,  2.7025e-01,  1.2179e-01,\n",
       "          2.2230e-01,  3.6062e-02, -8.9231e-01, -2.3419e-01,  4.5029e-01,\n",
       "         -5.3243e-01, -5.6019e-01, -2.4885e-01, -8.4486e-01, -2.0954e-01,\n",
       "         -4.2110e-01, -2.8626e-01,  2.6550e-01,  5.7030e-01, -5.9751e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_hook(self, module, input):\n",
    "  print(f\"Shape of input is {input.shape}\")\n",
    "\n",
    "model = models.resnet18()\n",
    "hook_ref  = model.fc.register_forward_hook(print_hook)\n",
    "model(torch.rand([1,3,224,224]))\n",
    "hook_ref.remove()\n",
    "model(torch.rand([1,3,224,224]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this code you should see text printed out showing the shape of the input to the linear classifier layer of the model. Note that the second time you pass a random tensor through the model, you shouldn’t see the print statement. \n",
    "\n",
    "When we add a hook to a module or tensor, PyTorch returns a reference to that hook. We should always save that reference (here we do it in hook_ref) and then call remove() when we’re finished. If you don’t store the reference, then it will just hang out and take up valuable memory (and potentially waste compute resources during a pass). Backward hooks work in the same way, except you call register_backward_hook() instead.\n",
    "\n",
    "Of course, if we can print() something, we can certainly send it to TensorBoard! Let’s see how to use both hooks and TensorBoard to get important stats on our layers during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_stats(i, module, input, output):\n",
    "    writer.add_scalar(f\"layer {i}-mean\", output.data.mean())\n",
    "    writer.add_scalar(f\"layer {i}-stddev\", output.data.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t use this by itself to set up a forward hook, but using the Python function partial(), we can create a series of forward hooks that will attach themselves to a layer with a set i value that will make sure that the correct values are routed to the right graphs in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "for i, m in enumerate(model.children()):\n",
    "    m.register_forward_hook(partial(send_stats, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we’re using model.children(), which will attach only to each top-level block of the model, so if we have an nn.Sequential() layer (which we will have in a ResNet-based model), we’ll attach a hook to only that block and not one for each individual module within the nn.Sequential list.\n",
    "\n",
    "If we train our model with our usual training function, we should see the activations start streaming into TensorBoard. \n",
    "You’ll have to switch to wall-clock time within the UI as we’re no longer sending step information back to TensorBoard with the hook (as we’re getting the module information only when the PyTorch hook is called)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1, accuracy = 0.96\n",
      "epoch 2\n",
      "Epoch 2, accuracy = 0.98\n",
      "epoch 3\n",
      "Epoch 3, accuracy = 0.98\n",
      "epoch 4\n",
      "Epoch 4, accuracy = 0.98\n",
      "epoch 5\n",
      "Epoch 5, accuracy = 0.99\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from functools import partial\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "trainset = datasets.MNIST(\"mnist_train\", train=True, download=True, transform=transform)\n",
    "train_data_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(train_data_loader))\n",
    "\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "writer.add_image(\"images\", grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cuda:0\"\n",
    "):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch {epoch+1}\")\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            ww, target = batch\n",
    "            ww = ww.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(ww)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            ww, target = batch\n",
    "            ww = ww.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(ww)\n",
    "            correct = torch.eq(torch.max(output, dim=1)[1], target).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        print(\"Epoch {}, accuracy = {:.2f}\".format(epoch+1, num_correct / num_examples))\n",
    "\n",
    "\n",
    "train(model, optimizer, criterion, train_data_loader, train_data_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mean is close to zero, but our standard deviation is also pretty close to zero as well. \n",
    "If this is happening in many layers of your network, it may be a sign that your activation functions (e.g., ReLU) are not quite suited to your problem domain. It might be worth experimenting with other functions to see if they improve the model’s performance; PyTorch’s LeakyReLU is a good alternative offering similar activations to the standard ReLU but lets more information through, which might help in training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
